import numpy as np
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix,classification_report

email=pd.read_csv('emails.csv')
#print(email.groupby('spam').describe())
email.drop_duplicates(inplace=True)
#print(email.groupby('spam').describe())

def data_pre(data):
    email['filtered_text'] = data.map(lambda text: text.split()[1:])
    stop_words = set(stopwords.words('english'))
    email['filtered_text'] = email['filtered_text'].map(lambda text: [w for w in text if w not in stop_words])
    email['filtered_text'] = email['filtered_text'].map(lambda text: ' '.join(text))
    email['filtered_text'] = email['filtered_text'].map(lambda text: re.sub('[^A-Za-z0-9]+', ' ', text))
    wnl = nltk.WordNetLemmatizer()
    email['filtered_text'] = email['filtered_text'].map(lambda text: wnl.lemmatize(text))
    return email['filtered_text']

cv=CountVectorizer()
fin_data=data_pre(email['text'])
x=cv.fit_transform(fin_data.values)
y=email['spam'].values
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=10)

def nva():
    nvb=MultinomialNB()
    nvb.fit(x_train,y_train)
    y_pred=nvb.predict(x_test)
    print(confusion_matrix(y_test,y_pred))
    print(classification_report(y_test,y_pred))

nva()