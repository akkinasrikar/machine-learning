import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression 
from sklearn.model_selection import GridSearchCV,train_test_split 
import matplotlib.pyplot as plt
from matplotlib import style
style.use('fivethirtyeight')

data=pd.read_csv('salary.csv')
x=np.array(data.iloc[:,0:1].values)
y=np.array(data.iloc[:,1:2].values)
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=6)

def sl():
    slr=LinearRegression()
    slr.fit(x,y)
    y_pred=slr.predict(x_test)
    plt.scatter(x,y,color='blue')
    plt.plot(x_test,y_pred,color='red')
    plt.show
    print(slr.intercept_)
    print(slr.coef_)
    
def normal_equation():
    x_new=np.c_[np.ones((30,1)),x]
    #print(x_new)
    m_and_c=np.linalg.inv(x_new.T.dot(x_new)).dot(x_new.T).dot(y)
    print(m_and_c)

def batch_gradient():
    eta=0.01    
    n=4000
    m=30
    x_new=np.c_[np.ones((30,1)),x]
    x_test_new=np.c_[np.ones((8,1)),x_test]
    m_and_c=np.random.rand(2,1)
    plt.scatter(x,y,color='blue')
    for i in range(n):
        gradient=(2/m)*x_new.T.dot(x_new.dot(m_and_c)-y)
        m_and_c=m_and_c-(eta*gradient)
        y_pred=x_test_new.dot(m_and_c)
        plt.plot(x_test,y_pred,color='red')
    print(m_and_c)
    plt.show
def stochastic_gradient_descent():
    n=5000
    t0,t1=30,50
    m=30
    x_new=np.c_[np.ones((30,1)),x]
    def learning_rate(t):
        return t0/(t1+t)
    m_and_c=np.random.rand(2,1)
    for i in range(n):
        for j in range(m):
            random_index=np.random.randint(m)
            xi=x_new[random_index:random_index+1]
            yi=y[random_index:random_index+1]
            gradient=(2/m)*xi.T.dot(xi.dot(m_and_c)-yi)
            eta=learning_rate(i*m+j)
            m_and_c=m_and_c-eta*gradient
    print(m_and_c)
    
    
sl()
print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
normal_equation()
print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
batch_gradient()
print(">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
stochastic_gradient_descent()

#output

[25792.20019867]
[[9449.96232146]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[[25792.20019867]
 [ 9449.96232146]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[[25792.19916813]
 [ 9449.96247438]]
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[[25011.29421135]
 [ 9582.48022305]]